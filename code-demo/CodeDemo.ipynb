{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "[This Notebook](https://github.com/UKDataServiceOpen/Synthetic-Data/blob/main/code-demo/CodeDemo.ipynb)\n",
    "\n",
    "Libraries:\n",
    "* [Mockaroo](https://www.mockaroo.com/)\n",
    "* [Python - Faker Documentation](https://faker.readthedocs.io/en/master/)\n",
    "* [R - Synthpop Documentation](https://www.synthpop.org.uk/resources.html)\n",
    "* [R - Synthpop Shiny App](https://synthpop.shinyapps.io/synthpop/)\n",
    "* [Synthpop - Further Reading](https://www.synthpop.org.uk/resources.html)\n",
    "\n",
    "Code Books:\n",
    "* [Binder - Python for those without a Python environment](https://mybinder.org/v2/gh/UKDataServiceOpen/Synthetic-Data/HEAD?filepath=code-demo%2FCodeDemo.ipynb)\n",
    "* [RPub - R for those without an R environment](https://rpubs.com/josephallen1994/778921)\n",
    "\n",
    "NATSAL:\n",
    "* [NATSAL](https://www.natsal.ac.uk/)\n",
    "* [UKDS Open teaching dataset and data dictionaries](https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=8786) \n",
    "* [Codebook](http://doc.ukdataservice.ac.uk/doc/8786/mrdoc/pdf/8786_natsal_open_codebook_feb21.pdf)\n",
    "* [Dataset Processing](https://github.com/UKDataServiceOpen/Synthetic-Data/blob/main/code-demo/NATSAL/NATSAL.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this Code Demo we will cover:\n",
    "* Introducing the NATSAL dataset\n",
    "* Introduction to Mockaroo - Web-based data generation and limitations\n",
    "* Introduction to Faker - Python-based data generation\n",
    "* Introduction to Synthpop - R-based data synthesis\n",
    "* Building our Synthesis pipeline\n",
    "\n",
    "We will mostly be working in Python, particularly the Pandas library of Python. I'll try to break this down.\n",
    "There will be a little bit of R.\n",
    "\n",
    "Time taken - TBD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "To start with we need to import any packages beyond base Python we are using.\n",
    "* Numpy - Support large multi-dimension arrays.\n",
    "* Pandas - Pythons data manipulation library, built on top of numpy.\n",
    "* Faker - Generates fake data.\n",
    "* Matplotlib - configure plots, and pandas plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any package isn't installed we can run pip within jupyter, this likely won't work in binder.\n",
    "# !pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATSAL Data\n",
    "NATSAL refers to the National Surveys of Sexual Attitudes and Lifestyles. \n",
    "[This](https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=8786) synthetic dataset holds some data on an individual-level identity, and opinions on various sexual topics.\n",
    "\n",
    "Let's perform some EDA(Exploratory Data Analysis):\n",
    "* Visually inspect rows.\n",
    "* Assess the quality of our data.\n",
    "* Begin to form a research questions.\n",
    "* Look for ways to simplify this dataset for ourselves.\n",
    "\n",
    "I've created a modification of the natsal data [here](https://github.com/UKDataServiceOpen/Synthetic-Data/blob/main/code-demo/NATSAL/natsal_3_teaching_open_with_personal.csv), with details on it's creation [here](https://github.com/UKDataServiceOpen/Synthetic-Data/blob/main/code-demo/NATSAL/NATSAL.ipynb).\n",
    "\n",
    "Anyway lets get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we read in our dataset, we make use of Pandas here (pd), and read in our object as a dataframe (denoted df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the head function, listing 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we read in our dataset, we make use of Pandas here (pd), and read in our object as a dataframe (denoted df)\n",
    "df = pd.read_csv('NATSAL/natsal_3_teaching_open_with_personal.csv')\n",
    "\n",
    "# call the head function, listing 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these rows make sense to us?\n",
    "\n",
    "The names and e-mails seem to add up. There appears to be some missing data let's look into that a little more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info summarises some basic qualities of our data.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes here:\n",
    "* we have 3799 rows, small enough that synthesis could be manual, but also that machine learning may not perform well\n",
    "* Most columns seem to have some missing data, age_at_first_child is mostly missing, but intentionally so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe gives mathematical information, not too useful in this dataset but always a step I recommend.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, respondants who had a first child, had them at 25 years old. \n",
    "\n",
    "Next I ask, what in here interests you? Can you draw out something you'd like to answer?\n",
    "To me being able to visualize the difference of oppinion between males and females, those with and without children, different ethnicities and different age groups all look very exciting.\n",
    "\n",
    "We have quite a lot of data that falls outside that scope, and I think it's worth dropping some. At this point i'd suggest:\n",
    "* renaming variables that don't make sense to you, i've already done this renaming dage1ch to age_at_first_child so at a glance we can recognise this data.\n",
    "* removing columns you aren't interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only columns we are interested in, I will drop the religion and age at first child\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to rename any columns we can do so here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only columns we are interested in, I will drop the religion and age at first child\n",
    "# Normally I would suggest redacting first_name, last_name and email at this point, but as an example we will keep them.\n",
    "df = df[['first_name', 'last_name', 'email', 'age_group', 'sex', 'ethnic_group', 'sexual_identity', 'relationship_status', 'has_child',\n",
    "       'opp_one_night_stand_ok', 'opp_sex_without_love_ok', 'opp_pressure_to_have_sex','opp_men_have_higher_sex_drive', 'opp_too_much_sex_media']]\n",
    "\n",
    "# If we want to rename any columns we can do so here.\n",
    "df.columns = ['first_name', 'last_name', 'email','age_group', 'sex', 'ethnic_group', 'sexual_identity', 'relationship_status', 'has_child',\n",
    "       'opp_one_night_stand_ok', 'opp_sex_without_love_ok', 'opp_pressure_to_have_sex','opp_men_have_higher_sex_drive', 'opp_too_much_sex_media']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify, what we are doing looks like redaction for disclosure control, we are dropping these columns becuase they currently don't add to our research.\n",
    "The simpler our dataset the simpler our synthesis is likely to become.\n",
    "If we were doing this for the sake of confidentiality it would be disclosure control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Breakdown\n",
    "So to break down our final dataset.\n",
    "* first_name\n",
    "* last_name\n",
    "* email\n",
    "* age_group - age group broken down into 9 year bands.\n",
    "* sex - appears to contain only Male or Female\n",
    "* ethnic_group - ethnic group, split as White or Not White\n",
    "* sexual_identity - sexual identity as Heterosexual or Not Heterosexual\n",
    "* relationship_status - relationship status\n",
    "* has_child - True or False for having a child\n",
    "* opp_one_night_stand_ok - Now we are getting into some oppinion data, this is the opinion on one night stands.\n",
    "* opp_sex_without_love_ok - Oppinion on sex, without love is ok.\n",
    "* opp_pressure_to_have_sex - Oppinion on People are under pressure to have sex\n",
    "* opp_men_have_higher_sex_drive - Oppinion on Men have a nturally higher sex drive than women.\n",
    "* opp_too_much_sex_media - Oppinion on Too much sex in\n",
    "\n",
    "## Visualizations\n",
    "There won't be much value in trying to visualize the names and e-mails, but lets get a quick look at who our respondants are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can count unique values using the value_counts() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can count unique values using the value_counts() function.\n",
    "df.age_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is less useful if we have a lot of different values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is less useful if we have a lot of different values.\n",
    "df.first_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot this object, calling .plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot this object, calling .plot()\n",
    "df.age_group.value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  .plot() assumes we want a line graph, this is not the case. We can control these plots using the kind parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .plot() assumes we want a line graph, this is not the case. We can control these plots using the kind parameter\n",
    "df.age_group.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems here:\n",
    "* This graph is small, probably hard to read\n",
    "* Looks a bit dull\n",
    "* the categories are clearly ordinal, that is 16-24 should probably be shown before 25-34 and so one\n",
    "* missing titles and axis labels\n",
    "\n",
    "We can configure matplotlib globally to resolve some of these for all future plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List availible styles\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a play and choose one you like\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set custom parameters\n",
    "plt.rcParams.update({'font.size': 22,\n",
    "                    'figure.figsize':(24,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .plot() assumes we want a line graph, this is not the case. We can control these plots using the kind parameter\n",
    "df.age_group.value_counts(sort=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_group'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a categorical type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default our plot tries to make something visually nice, sorting out bars in descending order.\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# make a category\n",
    "category_age = CategoricalDtype(\n",
    "    ['16-24', '25-34', '35-44', '45-54', '55-64', '65-74'], \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# replace our row with this category data\n",
    "df['age_group'] = df['age_group'].astype(category_age)\n",
    "\n",
    "# plot, but no longer sort descending as default\n",
    "df.age_group.value_counts(sort=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we can add a title with the title key in our plot function.\n",
    "df.age_group.value_counts(sort=False).plot(kind='bar', title=\"Frequency of Age groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see we have an overepresented population of under 35s, though we do have some older respondants represented.\n",
    "This could be very important to note later, poor training data in a model could mean we align closer with the more youthful oppinions on sexual attitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'].value_counts().plot(kind='bar', title=\"sex by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ethnic_group'].value_counts().plot(kind='bar', title=\"ethnic group by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sexual_identity'].value_counts().plot(kind='bar', title=\"sexuality by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relationship_status'].value_counts().plot(kind='bar', title=\"relationship status by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_child'].value_counts().plot(kind='bar', title=\"number of respondants with children\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_opp = CategoricalDtype(\n",
    "    ['Always wrong', 'Mostly wrong', 'Sometimes wrong', 'Rarely wrong', 'Not wrong at all', \"Depends/Don't know\"], \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# replace our row with this category data\n",
    "df['opp_one_night_stand_ok'] = df['opp_one_night_stand_ok'].astype(category_opp)\n",
    "\n",
    "df['opp_one_night_stand_ok'].value_counts(sort=False).plot(kind='bar', title=\"oppinions on 'are one night stands okay?' by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opp_sex_without_love_ok'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_agree = CategoricalDtype(\n",
    "    ['Agree strongly', 'Agree', 'Neither agree or disagree', 'Disagree', 'Disagree strongly', \"Don't know\"], \n",
    "    ordered=True\n",
    ")             \n",
    "\n",
    "# replace our row with this category data\n",
    "df['opp_sex_without_love_ok'] = df['opp_sex_without_love_ok'].astype(category_agree)\n",
    "\n",
    "df['opp_sex_without_love_ok'].value_counts(sort=False).plot(kind='bar', title=\"Do you agree with the statement 'Sex without love is okay' by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opp_pressure_to_have_sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace our row with this category data\n",
    "df['opp_pressure_to_have_sex'] = df['opp_pressure_to_have_sex'].astype(category_agree)\n",
    "\n",
    "df['opp_pressure_to_have_sex'].value_counts(sort=False).plot(kind='bar', title=\"Do you agree with the statement 'There is pressure to have sex' by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace our row with this category data\n",
    "df['opp_men_have_higher_sex_drive'] = df['opp_men_have_higher_sex_drive'].astype(category_agree)\n",
    "\n",
    "df['opp_men_have_higher_sex_drive'].value_counts(sort=False).plot(kind='bar', title=\"Do you agree with the statement 'Men have a higher sex drive than women?' by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also filter by sex here\n",
    "df[df['sex'] == 'Male']['opp_men_have_higher_sex_drive'].value_counts(sort=False).plot(kind='bar', title=\"Male - Do you agree with the statement 'Men have a higher sex drive than women?' by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also filter by sex here\n",
    "df[df['sex'] == 'Female']['opp_men_have_higher_sex_drive'].value_counts(sort=False).plot(kind='bar', title=\"Female - Do you agree with the statement 'Men have a higher sex drive than women?' by value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace our row with this category data\n",
    "df['opp_too_much_sex_media'] = df['opp_too_much_sex_media'].astype(category_agree)\n",
    "\n",
    "df['opp_too_much_sex_media'].value_counts(sort=False).plot(kind='bar', title=\"Do you agree with the statement 'There is too much sex in media' by value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclosure Control\n",
    "At this point we have some quite personal looking data.\n",
    "Special category data - ethnic group, sexual identity.\n",
    "We are also holding directly personal data - full name is quite bad, e-mail is worse!\n",
    "\n",
    "## Disclosure Control - Redaction\n",
    "We need to do something to protect these individuals. first name, last name and email all are unneeded for our research. In any non-demo situation I would drop these columns.\n",
    "Redaction - Removing data deemed too sensitive. Perhaps removing an entire row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could redact some columns using drop()\n",
    "# lets make some demo columns here to redact to make this clear\n",
    "\n",
    "df['first_name_demo'] = df['first_name']\n",
    "df['last_name_demo'] = df['last_name']\n",
    "df['email_demo'] = df['email']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could redact some columns using drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df = df.drop(columns=['first_name_demo', 'last_name_demo', 'email_demo'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclosure Control - Masking\n",
    "Masking - Replacing parts of the data that are sensitive, for example replacing names with synthetic names, initials or empty strings.\n",
    "This is a bit more difficult, but could be more useful. We can showcase that this data did have names, emails or more data without hiding it's existence entirely.\n",
    "If we want to do something as simple as Nulling out, that's quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild our demo columns\n",
    "df['first_name_demo'] = df['first_name']\n",
    "df['last_name_demo'] = df['last_name']\n",
    "df['email_demo'] = df['email']\n",
    "\n",
    "# print out the first 5 rows\n",
    "df[['first_name_demo', 'last_name_demo', 'email_demo']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can replace all of our first names with null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can replace all of our first names with null values\n",
    "df['first_name_demo'] = ''\n",
    "\n",
    "# print out the first 5 rows\n",
    "df[['first_name_demo', 'last_name_demo', 'email_demo']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Nulled out our data here, but maybe we could mask them with something a bit more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we replace values with the string FIRST_NAME, to many this is is clearly not the real data\n",
    "# A researcher may want first names, but should know the data provider to ask for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we replace values with the string FIRST_NAME, to many this is is clearly not the real data\n",
    "# A researcher may want first names, but should know the data provider to ask for more details\n",
    "df['first_name_demo'] = 'FIRST_NAME'\n",
    "df['email_demo'] = 'TEST@EMAIL.COM'\n",
    "\n",
    "# print out the first 5 rows\n",
    "df[['first_name_demo', 'last_name_demo', 'email_demo']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also shuffle these values around, while this will use the personal last_names, it can be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the entire last_name dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the entire last_name dataset\n",
    "df['last_name_demo'] = df['last_name_demo'].sample(frac=1).values\n",
    "\n",
    "# print out the first 5 rows\n",
    "df[['first_name_demo', 'last_name_demo', 'email_demo']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclosure Control - Coarsening\n",
    "We could also apply some coarsening here, while traditionally we would apply this to geographic or mathematical data with obvious rounding applications, we can apply it here to our last names.\n",
    "\n",
    "Pandas allows us to write custom functions for custom masking. For example, we could. The apply function is very useful for applying things like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coarsen to first character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset our demo columns\n",
    "df['first_name_demo'] = df['first_name']\n",
    "df['last_name_demo'] = df['last_name']\n",
    "df['email_demo'] = df['email']\n",
    "\n",
    "# return the first character as a function\n",
    "def get_initial(string):\n",
    "    return string[0]\n",
    "\n",
    "# we can then use apply\n",
    "df['first_name_demo'] = df['first_name_demo'].apply(get_initial)\n",
    "\n",
    "df[['first_name','first_name_demo', 'last_name_demo']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coarsen using a lambda function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use a shorthand lambda function\n",
    "df['last_name_demo'] = df['last_name_demo'].apply(lambda string: string[0])\n",
    "\n",
    "df[['first_name','first_name_demo', 'last_name', 'last_name_demo']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclosure Control - Mimicking\n",
    "We could instead make use of a data generation library such as Mockaroo or Faker. For now lets drop these demo columns, restoring us to our previously cleaned dataset.\n",
    "\n",
    "As mentioned in the webinars, the amount to which you should be reducing this data is entirely dependent on the dataset, your research question and the individual who approves you ethical use of data. There is no perfect solution here. \n",
    "For your own sake document the methods by which you are processing, synthetic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['first_name_demo', 'last_name_demo', 'email_demo'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mockaroo\n",
    "Next we will introduce [Mockaroo](https://www.mockaroo.com/) for substitution.\n",
    "\n",
    "## Task 1 - First name, last name, email and gender\n",
    "* We can add and remove further columns, add gender\n",
    "* we can re-order if needed\n",
    "\n",
    "note: e-mail is connected with first name and last name.\n",
    "lost e-mail provider, an excellent feature in fraud detection.\n",
    "Names don't correlate here with age or gender.\n",
    "showcase gendered names.\n",
    "\n",
    "## Task 2 - E-mails are missing 2% of the time\n",
    "* We can set a missing percentage, for example e-mail was missing 2%. Some people don't have e-mail addresses such as young children, or the elderly!\n",
    "\n",
    "## Task 3 - Maths\n",
    "* create a column - weight\n",
    "* find a mathematical dsitribution - normal with a mean of 55 and std of 5.\n",
    "\n",
    "## Task 4 - Height\n",
    "* create a height column\n",
    "* use built-in scripting to mimick height - field('weight'\n",
    "* use built-in scripting to sum two columns -  field('weight') + this\n",
    "* add some noise - field('weight') + this + 50 or + random(40, 60)\n",
    "\n",
    "## Task 5 - Tidy up\n",
    "* Really we don't need a lot of this though, so let's remove gender, weight and height.\n",
    "* We can preview the first 100 rows, and download our data\n",
    "* How many rows do we need?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* so let's generate 3799\n",
    "* Uh oh... this is where Mockaroo makes their money and is now a limitation of the technology\n",
    "* We can get around this by generating multiple datasets and merging them.\n",
    "* If you are using the binder link you won't be able to add your new datasets so I have provided some already, split by gender under:\n",
    "    * NATSAL/mockaroo.csv\n",
    "    * NATSAL/male.csv\n",
    "    * NATSAL/male_2.csv\n",
    "    * NATSAL/female.csv\n",
    "    * NATSAL/female_2.csv\n",
    "    * NATSAL/female_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mockaroo = pd.read_csv('NATSAL/mockaroo.csv')\n",
    "mockaroo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join these datasets with no logic, called concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join this with our dataset\n",
    "result = pd.concat([mockaroo, df], axis=1, join=\"inner\")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the head function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may see some conflicts, these names arguably conflict with sex, or in more nuanced ways age and ethnicity. How can we correlate with names?\n",
    "\n",
    "To start with, let's correlate only with Male or Female.\n",
    "We need to generate multiple datasets for male and female data in mockaroo here.\n",
    "We can split the dataset, and merge in these new names. This is simple enough to do with only Male and Female. We split into two datasets, merge in our anonymised data, merge those datasets back together and resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sex\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original names and email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can split our dataframe into male and female only, and generate datasets in Mockaroo for each of these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original names and email\n",
    "df = df.drop(columns=['first_name', 'last_name', 'email'])\n",
    "\n",
    "# We can split our dataframe into male and female only, and generate datasets in Mockaroo for each of these\n",
    "male_df = df[df['sex'] == 'Male']\n",
    "female_df = df[df['sex'] == 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "male_personal_df_1 = pd.read_csv('NATSAL/male.csv')\n",
    "male_personal_df_2 = pd.read_csv('NATSAL/male_2.csv')\n",
    "\n",
    "# concat 1000 rows from Mockaroo\n",
    "male_personal_df = pd.concat([male_personal_df_1,male_personal_df_2])\n",
    "\n",
    "# match the size of our dataset\n",
    "male_personal_df = male_personal_df[0:1534]\n",
    "\n",
    "# reset indexes so we can concatenate without problems\n",
    "male_personal_df.reset_index(drop=True, inplace=True)\n",
    "male_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# merge datasets\n",
    "male_df = pd.concat([male_personal_df, male_df], axis=1,  ignore_index=False)\n",
    "male_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with female\n",
    "# import datasets\n",
    "female_personal_df_1 = pd.read_csv('NATSAL/female.csv')\n",
    "female_personal_df_2 = pd.read_csv('NATSAL/female_2.csv')\n",
    "female_personal_df_3 = pd.read_csv('NATSAL/female_3.csv')\n",
    "\n",
    "# concat 1000 rows from Mockaroo\n",
    "female_personal_df = pd.concat([female_personal_df_1, female_personal_df_2, female_personal_df_3])\n",
    "\n",
    "# match the size of our dataset\n",
    "female_personal_df = female_personal_df[0:2265]\n",
    "\n",
    "# reset indexes so we can concatenate without problems\n",
    "female_personal_df.reset_index(drop=True, inplace=True)\n",
    "female_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# merge datasets\n",
    "female_df = pd.concat([female_personal_df, female_df], axis=1,  ignore_index=False)\n",
    "female_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we need to merge all that data back together\n",
    "\n",
    "\n",
    "# shuffle so our data isn't split into male and female halves, a shortcut here is to sample all of our rows, and make use of 1, or 100% of the rows\n",
    "\n",
    "# reindex\n",
    "\n",
    "# head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we need to merge all that data back together\n",
    "personal_df = pd.concat([female_df, male_df])\n",
    "\n",
    "# shuffle so our data isn't split into male and female halves, a shortcut here is to sample all of our rows, and make use of 1, or 100% of the rows\n",
    "personal_df = personal_df.sample(frac=1)\n",
    "\n",
    "# reindex\n",
    "personal_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "personal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop demos if still here\n",
    "# personal_df = personal_df.drop(columns=['first_name_demo', 'last_name_demo', 'email_demo'])\n",
    "\n",
    "# rename synthetic columns\n",
    "personal_df.columns = ['synth_first_name', 'synth_last_name', 'synth_email','age_group', 'sex', 'ethnic_group', 'sexual_identity', 'relationship_status', 'has_child',\n",
    "       'opp_one_night_stand_ok', 'opp_sex_without_love_ok', 'opp_pressure_to_have_sex','opp_men_have_higher_sex_drive', 'opp_too_much_sex_media']\n",
    "\n",
    "personal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a very nice first pass, and lets us get some quite complicated simulation done. I would suggest if you feel technically capable of this, try to reproduce what you are doing ina  programming language of your choice, this system is only a fraction of what you really have the power to do in a language like Python.\n",
    "\n",
    "What we've seen here is that although we have masked the names and emails, we have a problem. these new e-mails look slightly off for now.\n",
    "\n",
    "Simply by correlating names with gender, we've had to generate 5 datasets, merge them together and we've split our entire dataset into 2. If we do this with sex and ethnic group we have 4 different datasets. \n",
    "\n",
    "including age groups there are now 24 different datasets. That is only to correlate with our one name variable, really we need to correlate between all variables!\n",
    "\n",
    "What if we had 1,000,000 rows?\n",
    "Do you want to sit there generating 1000 datasets?\n",
    "Do you want to deal with all those merge changes? I know I don't!\n",
    "the complexity of this generation compounds. What we are effectively doing here is creating a very basic version of a decision tree.\n",
    "\n",
    "Let's make use of Faker to churn this out a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faker\n",
    "Next up we are looking at [Faker](https://faker.readthedocs.io/en/master/).\n",
    "Faker is a Python package that generates fake data for you.\n",
    "\n",
    "In binder you should already have access to this package,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise Faker\n",
    "\n",
    "# call the name function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we initialise a faker object\n",
    "fake = Faker()\n",
    "\n",
    "# here we can call Fakers various providers, and we can rerun\n",
    "fake.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the addresses function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even fake addresses\n",
    "fake.address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 names in a for loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate as many names as we need.\n",
    "for _ in range(10):\n",
    "  print(fake.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faker has some built-in providers listed [here](https://faker.readthedocs.io/en/stable/providers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barcodes\n",
    "print(fake.ean())\n",
    "\n",
    "# free emails like hotmail, gmail etc\n",
    "print(fake.ascii_free_email())\n",
    "\n",
    "# full credit card details\n",
    "print(fake.credit_card_full())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond this, Faker also has community providers, meaning anybody can write anything we need. There exists seperate packages to generate mark down blog posts, air travel, credit card scores and more.\n",
    "[Community Providers](https://faker.readthedocs.io/en/stable/communityproviders.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional providers, we need emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional providers, we need emails\n",
    "from faker.providers import internet\n",
    "\n",
    "fake = Faker()\n",
    "fake.add_provider(internet)\n",
    "\n",
    "print(fake.ipv4_private())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perk over mockaroo is that we can make use of different locales, meaning we can generate individuals from other locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Locales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate some italian names for example\n",
    "fake = Faker('it_IT')\n",
    "for _ in range(10):\n",
    "    print(fake.name())\n",
    "    print(fake.address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to multiple locales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or event a generate a combination of different locales. re running generates new names\n",
    "fake = Faker(['it_IT', 'en_US', 'ja_JP'])\n",
    "for _ in range(10):\n",
    "    print(fake.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it reproducible with a seed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also seed our generation for consistent results, re running keeps our results identical.\n",
    "# If you need to prove you generated these names, here is how you do it.\n",
    "\n",
    "fake = Faker('en_UK')\n",
    "Faker.seed(4321)\n",
    "for _ in range(10):\n",
    "    print(fake.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emails are built into faker so we can use those directly, what happens when we generate the users we need for the NATSAL data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker(['it_IT', 'en_US', 'ja_JP'])\n",
    "# Generating all three, one at a time creates three seperate individuals.\n",
    "print(fake.first_name_female())\n",
    "print(fake.last_name())\n",
    "print(fake.ascii_free_email())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom provider to change based on White or not white, male or female\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate all three together, we can make our own provider.\n",
    "# first, import a similar Provider or use the default one\n",
    "from faker.providers import BaseProvider\n",
    "fake = Faker('en_UK')\n",
    "\n",
    "# create new provider class. really we can do anything Python lets us here.\n",
    "class MyProvider(BaseProvider):\n",
    "    def individual(self, sex, ethnic_group):\n",
    "        \n",
    "        # This is a HUGE oversimplification of the complciated relationship between names, sex and race.         \n",
    "        # conditional decisions now possible, we can split male and female generation\n",
    "        if ethnic_group == \"White\":\n",
    "            fake = Faker('en_UK')\n",
    "        else:\n",
    "            fake = Faker('es_ES')\n",
    "        \n",
    "        if sex == \"Male\":\n",
    "            first_name = fake.first_name_male()\n",
    "        else:\n",
    "            first_name = fake.first_name_female()\n",
    "\n",
    "        \n",
    "        last_name = fake.last_name()\n",
    "        \n",
    "        # We make a big assumption here, that all emails are a first name first character, prepended to a last name\n",
    "        email = first_name[0] + last_name + '@' + fake.free_email_domain()\n",
    "        \n",
    "        \n",
    "        return {'first_name': first_name,\n",
    "                'last_name': last_name,\n",
    "                'email': email,\n",
    "               }\n",
    "\n",
    "fake.add_provider(MyProvider)\n",
    "\n",
    "# Generate a male\n",
    "fake.individual('Male', 'White')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.individual('Male', 'Not white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a Female\n",
    "fake.individual('Female', 'White')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.individual('Female', 'Not white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can return back to our original dataset, we have almost 4000 users that all need new names and emails generated. This is a very simple case, if we had millions of rows, and much more complicated individuals we can go a lot deeper and Faker over Mockaroo is the tool to do this.\n",
    "\n",
    "First let's deal with those synthetic e-mails, something isn't quite right there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our new columns\n",
    "df['synth_first_name'] = 'FIRST_NAME'\n",
    "df['synth_last_name'] = 'LAST_NAME'\n",
    "df['synth_email'] = 'TEST@EMAIL.com'\n",
    "\n",
    "# rearrange columns\n",
    "df = df[['synth_first_name', 'synth_last_name', 'synth_email', 'age_group', 'sex', 'ethnic_group', 'sexual_identity', 'relationship_status', 'has_child',\n",
    "       'opp_one_night_stand_ok', 'opp_sex_without_love_ok', 'opp_pressure_to_have_sex','opp_men_have_higher_sex_drive', 'opp_too_much_sex_media']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we applied the apply function before we could only use one variable at a time, but apply is more powerful than this\n",
    "df['age_group'].apply(lambda age: age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function and apply it to the entire row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 seconds to run, 5 per 100 rows\n",
    "# we can instead use apply on the entire row, I won't use lambda here for readability\n",
    "def generate_individual(row):\n",
    "    individual = fake.individual(row['sex'], row['ethnic_group'])\n",
    "\n",
    "    row['synth_first_name'] = individual['first_name']\n",
    "    row['synth_last_name'] = individual['last_name']\n",
    "    row['synth_email'] = individual['email']\n",
    "\n",
    "    return row\n",
    "    \n",
    "# note axis = 1, as in apply to rows not columns\n",
    "# For the sake of speed I am only running this on the first 500 rows\n",
    "df[:50] = df[:50].apply(generate_individual, axis=1)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faker - Use case\n",
    "Look across these rows, does anything not make sense? first names correlate with gender, and ethnic group.\n",
    "Emails are comprised of first names and last names, though the variety is quite poor.\n",
    "\n",
    "What we are finding here is that as we anonymise in more detail, the complexity of this generation compounds. \n",
    "What we are effectively doing here is creating a very basic version of a decision tree.\n",
    "\n",
    "# Decision Tree\n",
    "[Source](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7)\n",
    "\n",
    "A decision tree is very similar to the flowcharts we are used to seeing, for example,\n",
    "![image.png](https://miro.medium.com/max/531/1*rmV_02XSjCpCaj11wZJDCQ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world we might summarise a lot of decisions we make as boiling down to one or two factors. \n",
    "We might say we drive to work when the weather is bad, but really it could be a lot more complicated than that.\n",
    "\n",
    "Or is it more complicated than that?:\n",
    "* Do we have a job?\n",
    "* Is the weather nice?\n",
    "* Is the public transport good?\n",
    "* Is it affordable?\n",
    "* Do I have a car?\n",
    "* Is the car working?\n",
    "\n",
    "All these factors are important to our decision, though some are more important than others. \n",
    "1 day in a year of data our road might be blocked by a fallen tree.\n",
    "\n",
    "We can build a tree that can deal with even these edge cases, but often these trees are huge, complicated, hard to understand and general what we call \"overfit\" in Machine learning. Where our model works very well on our training data, but not in the real situation.\n",
    "\n",
    "these trees can get a bit more complicated,case we can have more complicated decisions like this. When trained with a Machine Learning alogirhtm we start to get very specific and strange numbers.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/781/1*fGX0_gacojVa6-njlCrWZw.png)\n",
    "\n",
    "\n",
    "While we could train a decision tree on our data, this won't do much to protect our individuals, an overfit decision tree might just learn to churn out the same individual every time. We could add noise and go through general disclosure control. Or we could use a data synthesis package and not worry about this.\n",
    "\n",
    "Our tree might learn to predict names with frequency, and when there is personal data on the mix this won't work well.\n",
    "\n",
    "Decision tree algorithms are how Synthpop works, among some other tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthpop\n",
    "While we could train a decision tree on our data, this won't do much to protect our individuals, an overfit decision tree might just learn to churn out the same individual every time. We could add noise and go through general disclosure control. Or we could use a data synthesis package and not worry about this.\n",
    "\n",
    "[R - Synthpop Shiny App](https://synthpop.shinyapps.io/synthpop/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faker, Synthpop and Mockaroo feature comparison. from ONS\n",
    "https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsmethodologyworkingpaperseriesnumber16syntheticdatapilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Setup\n",
    "To setup R we need to set our working directory\n",
    "\n",
    "We need to at least import the two libraries we need\n",
    "* readr - to read in our dataset\n",
    "* synthpop - to synthesize our data\n",
    "\n",
    "Alternatively you can follow along in our [RPubs](https://rpubs.com/josephallen1994/778921)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R Launch Details\n",
    "# setwd(\"~/GitHub/Synthetic-Data/code-demo\")\n",
    "\n",
    "# import library\n",
    "# library(readr)\n",
    "# library(synthpop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R - Next we read in our data\n",
    "# df <- read_csv(\"NATSAL/natsal_3_teaching_open_with_personal.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthpop documentation suggests not trying to synthesize more than around 12 variables.\n",
    "## Synthpop Best Practices\n",
    "* Around 12 variables maximum - we have 14, 11 are factors.\n",
    "* Do not train on less than 500 rows - we have 3799\n",
    "* run codebook.syn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R\n",
    "# codebook.syn(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthpop codebook\n",
    "* nmiss is missing values\n",
    "* perctmiss - percentage missing\n",
    "* ndisctinct, the number of distinct values.\n",
    "\n",
    "Now our personal data is almost always unique, this means a decision tree might try to use it to say \"If a user is named Carlos, they must hast be for one night stands, think sex without love is okay, be 25 years old\" and so on. this is what we call overfitting, our tree could end up with thousands of nodes representing this granularity and would be quite useless applied elsewhere. Which leads us to our next best practice.\n",
    "\n",
    "## Synthpop Best Practices\n",
    "* Remove any identifying data\n",
    "\n",
    "In fact, if we try to skip these steps and go straight to synthesis we get a helpful error.\n",
    "`Error: Factor(s) with more than 60 levels: first_name (3026), last_name (3659), email (3704)`\n",
    "\n",
    "we can bypass this if we want, but I think it's valid, let's drop these columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifying columns\n",
    "# drops <- c(\"first_name\",\"last_name\",\"email\", 'importance_religion', 'age_at_first_child')\n",
    "# df <- df[ , !(names(df) %in% drops)]\n",
    "\n",
    "# Logical also need to be converted to factors\n",
    "# df$has_child <- as.factor(df$has_child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthpop Best Practices\n",
    "* Change character text into factors\n",
    "\n",
    "We can convert all character columns to factors with this one line.\n",
    "`df <- as.data.frame(unclass(df), stringsAsFactors = TRUE)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthpop Best Practices\n",
    "* beware non R missing value NA, for example nulls, False and other \"missing\" values. It looks like we are fine here as we saw missing valuessyn_df\n",
    "* Remove any variables that could be dervies from others.\n",
    "* Set any rules, children shouldn't have a job etc. You can set the rules with the parameters rules  and rvalues of the syn() function. The syn() function will warn you if the rule is not obeyed in the observed data.\n",
    "\n",
    "So at this stage let's give synthesis a go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R Synthesis, synthesize 1 dataset1\n",
    "# syn_df <- syn(df)\n",
    "\n",
    "# R Synthesis, synthesize 5 datasets\n",
    "# syn_df <- syn(df, m = 5)\n",
    "\n",
    "# Summarise\n",
    "#  summary(syn_df)\n",
    "    \n",
    "# We can then compare these, with multiple datasets\n",
    "# compare(syn_df, df) \n",
    "\n",
    "# compare a single variable\n",
    "# compare(syn_df, df, vars = \"age_group\")\n",
    "\n",
    "# or compare a single variable, across all synthesized datasets.\n",
    "# compare(syn_df, df, vars = \"age_group\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"sex\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"ethnic_group\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"sexual_identity\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"relationship_status\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"has_child\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"opp_one_night_stand_ok\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"opp_sex_without_love_ok\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"opp_pressure_to_have_sex\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"opp_men_have_higher_sex_drive\", msel = 1:5)\n",
    "# compare(syn_df, df, vars = \"opp_too_much_sex_media\", msel = 1:5)\n",
    "\n",
    "# of course it's not enough simply to match the counts, we need to see if things correlate\n",
    "# Let's write this data out so we can read it in here.\n",
    "# write.syn(syn_df,\"syn_NATSAL\", filetype = \"csv\", convert.factors = \"numeric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let's read in this dataset into Python and see if we can compare these further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in this dataset\n",
    "syn_df = pd.read_csv('syn_NATSAL.csv')\n",
    "syn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that looks a little off here is our has_child, which has been converted from a boolean True or False to numeric labels.\n",
    "\n",
    "let's resolve this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_df['has_child'] = syn_df['has_child'].replace(1,False)\n",
    "syn_df['has_child'] = syn_df['has_child'].replace(2,True)\n",
    "\n",
    "syn_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need our first names, last names and emails. Again I am only going to do this for a few rows for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our new columns\n",
    "syn_df['synth_first_name'] = 'FIRST_NAME'\n",
    "syn_df['synth_last_name'] = 'LAST_NAME'\n",
    "syn_df['synth_email'] = 'TEST@EMAIL.com'\n",
    "\n",
    "# rearrange columns\n",
    "syn_df = syn_df[['synth_first_name', 'synth_last_name', 'synth_email', 'age_group', 'sex', 'ethnic_group', 'sexual_identity', 'relationship_status', 'has_child',\n",
    "       'opp_one_night_stand_ok', 'opp_sex_without_love_ok', 'opp_pressure_to_have_sex','opp_men_have_higher_sex_drive', 'opp_too_much_sex_media']]\n",
    "\n",
    "syn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_df[:50] = syn_df[:50].apply(generate_individual, axis=1)\n",
    "\n",
    "syn_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's do some comparison between this and our real data, for this perhaps some notion of correlation for factoral data makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['opp_one_night_stand_ok'].hist(alpha=1)\n",
    "syn_df['opp_one_night_stand_ok'].hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.apply(lambda x : pd.factorize(x)[0]).corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_corr = syn_df.apply(lambda x : pd.factorize(x)[0]).corr()\n",
    "syn_corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthpop - summary\n",
    "We can see here that our first names correlate largely with our last names and email. This isn't suprising as we never generated a full dataset so they are still mostly \"FIRST_NAME\", \"LAST_NAME\" and \"TEST@EMAIL.COM\".\n",
    "All variables correlate with themselves.\n",
    "\n",
    "The strongest correlations are between age, relationship status, child status.\n",
    "There also seems to be some relationship between ethnic group and sexual_identity.\n",
    "It also seems that one gender seems to be more likely to have a child than another.\n",
    "\n",
    "Some of these correlations have remained close such as the relationship between age_group and having a child.\n",
    "child and relationship status have changed quite a lot, dropping from a tight correlation of 0.4 down to 0.26\n",
    "\n",
    "What we do have at the end of this process though, is a colelction of connected oppinion data, with disclosure control used to make our data look the same as the original. Analysis could be applied to our synthetic data to test our methods while we wait for approved access to the original data. \n",
    "\n",
    "In this case the starting data was also synthetic, but now that we can test our methods without needing the personal data we could even hand over a notebook like this to the data provider, and hope they will apply the data themselves.\n",
    "\n",
    "## Synthpop - Next steps\n",
    "outside of this demo check out these [advanced resources](https://www.synthpop.org.uk/resources.html)\n",
    "\n",
    "Now you have managed your first synthesis you could read our paper in the Journal of Statistical Software and explore other resources on our website with more explanation of different features of synthpop including:\n",
    "* statistical disclosure control functions,\n",
    "* customising your synthesis by defining methods, order and predictor matrix,\n",
    "* evaluating the utility of the synthetic data,\n",
    "* comparing model fits between observed and synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "To conclude:\n",
    "- Exploratory data analysis is needed to understand dataset context.\n",
    "- From here Mockaroo can do the job of basic masking.\n",
    "- If a dataset is too large we can write a custom Faker provider.\n",
    "- Synthpop is quite easy to use, but has very deep and academic style tutorials.\n",
    "\n",
    "Any questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
